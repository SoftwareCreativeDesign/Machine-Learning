# -*- coding: utf-8 -*-
"""Creat_Model_And_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S81maZ6VIon_j8Sbbm1VBo9kKSSswj8I
"""

# Reference

# PyCaret (https://pycaret.org/)
# Tutorial (https://colab.research.google.com/github/pycaret/pycaret/blob/master/tutorials/Tutorial%20-%20Binary%20Classification.ipynb#scrollTo=0d92512d)

# File Path

# <Test> /content/drive/MyDrive/DaeguCatholicUniversity_Software_Creative_Design_01/Test_DataSet_LeeSioen_2024.11.27(AM_10_45)_(100,001).csv
# <Final> /content/drive/MyDrive/DaeguCatholicUniversity_Software_Creative_Design_01/Final_DataSet_LeeSioen_2024.11.27(AM_10_06)_(862,465).csv

# Mount Google Drive and install necessary libraries

from google.colab import drive
drive.mount('/content/drive')

# Install PyCaret

!pip install pycaret[full] --quiet

# Import necessary libraries

import os
import pickle
import pandas
import numpy
from pycaret.classification import *
from sklearn.model_selection import train_test_split

# Load the dataset

data_path = '/content/drive/MyDrive/DaeguCatholicUniversity_Software_Creative_Design_01/Test_DataSet_LeeSioen_2024.11.27(AM_10_45)_(100,001).csv'
data = pandas.read_csv(data_path)

# Add a synthetic target column if it does not exist
# Uncomment the following line if your dataset lacks a target column
# data['type'] = numpy.random.choice([0, 1], size=len(data))  # Random binary labels

# Set the target column
target_column = 'type'  # Replace with the actual target column name

# Display basic information about the dataset

print(data.head())  # Display the first few rows of the dataset
print(data.info())  # Display data types and null values

# Randomly split the dataset into train and test subsets

train_data, test_data = train_test_split(data, test_size=0.2, random_state=None)

# Initialize the PyCaret setup for the train dataset

clf1 = setup(
    data=train_data,
    target=target_column,
    session_id=None,  # No fixed random seed to ensure random splitting
    normalize=True,  # Scale the data
    fix_imbalance=True  # Handle class imbalance if present
)

# Compare baseline models to find the best one

best_model = compare_models()

# Check model details

best_model

# Evaluate the best model on the test dataset

best_model_results = predict_model(best_model, data=test_data)

# # Check the head of tuned_test_results to confirm the predicted column name

# print(best_model_results.head())

# Ensure 'prediction_label' exists in the result and calculate accuracy

best_model_accuracy = best_model_results['prediction_label'].eq(test_data[target_column]).mean()
print(f"Accuracy of the best model: {best_model_accuracy:.4f}")

# Perform hyperparameter tuning on the best model

tuned_model = tune_model(best_model)

# Check model details

tuned_model

# Evaluate the tuned model on the test dataset

tuned_model_results = predict_model(tuned_model, data=test_data)

# Evaluate the tuned model on the test dataset

tuned_accuracy = tuned_model_results['prediction_label'].eq(test_data[target_column]).mean()
print(f"Accuracy of the tuned model: {tuned_accuracy:.4f}")

# Use AutoML to find the best final model

automl_model = automl(optimize='Accuracy')  # Find the best model according to accuracy

# Check model details

automl_model

# Evaluate the AutoML-selected model on the test dataset

automl_model_results = predict_model(automl_model, data=test_data)

# Evaluate the tuned model on the test dataset

automl_accuracy = automl_model_results['prediction_label'].eq(test_data[target_column]).mean()
print(f"Accuracy of the AutoML model: {automl_accuracy:.4f}")

# Save model with a specific path
model_save_path = '/content/drive/MyDrive/DaeguCatholicUniversity_Software_Creative_Design_01'

# Calculate and compare models
if automl_accuracy == tuned_accuracy and automl_model.__class__ == tuned_model.__class__:
    print("AutoML-selected model and tuned model are identical in accuracy and type. Skipping blending...")

    # Save the best model based on accuracy
    final_model_to_save = automl_model
    print("AutoML-selected model will be saved as it has identical accuracy.")
else:
    print("AutoML-selected model and tuned model differ. Proceeding to blend models...")

    # Blend models
    blended_model = blend_models(estimator_list=[tuned_model, automl_model])

    # Save the blended model
    final_model_to_save = blended_model
    print("Blended model will be saved.")

# Save the chosen model using pickle
model_filename = os.path.join(model_save_path, 'URL_Classification_Model_from_Undecided.pkl')
with open(model_filename, 'wb') as file:
    pickle.dump(final_model_to_save, file)
print(f"Model saved successfully at {model_filename}.")

# Load the model back for verification
with open(model_filename, 'rb') as file:
    loaded_model = pickle.load(file)
print("Model loaded successfully for verification.")

# # Load the saved model for inference

# loaded_model = load_model('url_classification_model_from_Undecided')
# print("Model loaded successfully.")

# # Evaluate the loaded model on the test dataset

# loaded_test_results = predict_model(loaded_model, data=test_data)
# loaded_model_accuracy = loaded_test_results['Label'].eq(test_data[target_column]).mean()
# print(f"Accuracy of the loaded model: {loaded_model_accuracy:.4f}")

# Create API from the saved model

create_api('url_classification_model_from_Undecided', api_name = 'url_classification_api')

# Instructions

# (1) To run the API locally, use the command :
#     uvicorn url_classification_api:app --host 0.0.0.0 --port 8000
# (2) After running, the API will be available at :
#     http://127.0.0.1:8000
# (3) Swagger UI for testing :
#     http://127.0.0.1:8000/docs

# # Create Dockerfile for the saved model

# create_docker('url_classification_model_from_Undecided', 'url_classification_docker')

# # Instructions

# # (1) Navigate to the folder containing the generated Dockerfile (`url_classification_docker`).
# # (2) Build the Docker image using the command :
# #     docker build -t url_classification_image .
# # (3) Run the Docker container using :
# #     docker run -d -p 8000:8000 url_classification_image
# # (4) Access the API at :
# #     http://127.0.0.1:8000
# # (5) Test using Swagger UI :
# #     http://127.0.0.1:8000/docs